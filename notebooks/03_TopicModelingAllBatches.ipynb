{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "- This notebook walks thru a topic modeling process using `data/interim/subset_first_15000.gzip` \n",
    "- At the end of the notebook, a labeled data will be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory\n",
    "import os\n",
    "os.chdir(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function to process raw data by chunks \n",
    "(Functions from `data_prep` package)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jhonsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.data_prep.topic_modeling_helpers import (preprocess_text, make_corpus,extract_labels,\n",
    "                                                  find_best_crime_topic,build_lda_model, \n",
    "                                                  extract_labels)\n",
    "from src.data_prep.preprocessing_helpers import impute_nans, remove_empty_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chunk:\n",
    "- preprocess text file (remove empty articles, impute nans)\n",
    "- topic model it\n",
    "- find best topic modeled as \"crime\" \n",
    "    - keywords: ['black', 'police', 'violence', 'kill', 'arrest']\n",
    "- save each labeled news into `data/interim` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTs (Hyperparameters) from previous notebook (TopicModelingFirstBatch.ipynb)\n",
    "ALPHA = 'asymmetric'\n",
    "ETA = 1\n",
    "NTOPICS = 14\n",
    "\n",
    "# Helper functions\n",
    "def create_topic_model(dataset, start_row, end_row):\n",
    "\n",
    "    # Preprocess \n",
    "    papers = dataset['article'].apply(preprocess_text)\n",
    "    # Prepare topic modeling input\n",
    "    corpus, id2word, bigrams, data_lemmatized = make_corpus(papers)\n",
    "    # Build model & print the topic number with best matching keywords\n",
    "    lda_model = build_lda_model(corpus, id2word, n_topics=NTOPICS, alpha=ALPHA, eta=ETA)\n",
    "    best_topic_no = find_best_crime_topic(lda_model, n_topics=NTOPICS)\n",
    "    # label document\n",
    "    dataset['topic'] = extract_labels(lda_model, data_lemmatized, corpus, n_topics=NTOPICS)\n",
    "    crime_subset = dataset[dataset.topic==best_topic_no]\n",
    "    \n",
    "    # Save subset of crime (labeled) file\n",
    "    filename = f'labeled_crime_row{start_row}_to_row{end_row}.gzip'\n",
    "    filepath = os.path.join('data', 'interim', filename)\n",
    "    crime_subset.to_parquet(filepath, compression='gzip')\n",
    "    print(f'{filename} has ', crime_subset.shape[0], ' rows')\n",
    "    return crime_subset, best_topic_no, filename\n",
    "        \n",
    "def process_chunk(dataset, start_row, end_row):\n",
    "    return create_topic_model(impute_nans(remove_empty_articles(dataset)), start_row, end_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing each chunk\n",
    "**BEWARE** This takes **13 hours** to run locally! \n",
    "  \n",
    "Output:  \n",
    "- `data/interim/labeled_crime_row{start_row}_to_row{end_row}.gzip`\n",
    "- `data/interim/crime_topic_index.gzip` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'data'\n",
    "raw_filepath = os.path.join(os.path.relpath('.'), 'data', 'raw', 'all-the-news-2-1.csv')\n",
    "\n",
    "start_row = 1\n",
    "chunksize = 20000  # This is equivalent to <25 mb parquet file\n",
    "end_row = chunksize \n",
    "\n",
    "def start():\n",
    "    confirm = input(\"do you want to start? [y/n]\")\n",
    "    if (confirm == 'y') or (confirm =='Y'):\n",
    "\n",
    "        all_crime_news = pd.DataFrame()\n",
    "        crime_topic_index = pd.DataFrame()\n",
    "        for chunk in pd.read_csv(raw_filepath, header=0,\n",
    "                                 chunksize=chunksize, \n",
    "                                 encoding='utf-8',\n",
    "                                 usecols = [\"date\",\"author\",\"title\",\"publication\",\"section\",\"url\", \"article\"],\n",
    "                                 parse_dates=['date']\n",
    "                                ):\n",
    "            crime_news, best_topic_no, fname = process_chunk(chunk, start_row, end_row)\n",
    "\n",
    "            crime_topic_index = crime_topic_index.append(pd.DataFrame({'filename': fname, 'topic': best_topic_no,\n",
    "                                                                      'start_row': start_row, 'end_row': end_row},\n",
    "                                                                     columns=['filename','topic','start_row','end_row'], index=[0]), ignore_index=True)\n",
    "            all_crime_news = all_crime_news.append(crime_news, ignore_index=True)\n",
    "\n",
    "            #print(f'\\t===== Finished with first {end_row} rows ====\\n')\n",
    "            start_row += chunksize\n",
    "            end_row += chunksize\n",
    "\n",
    "        filepath = os.path.join('data', 'processed', 'crime_topic_index.gzip')\n",
    "        crime_topic_index.to_parquet(filepath, compression='gzip')\n",
    "    \n",
    "    print('complete')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Un-comment below to start! ########\n",
    "# start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}